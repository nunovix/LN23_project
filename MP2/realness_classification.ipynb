{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processed_train/X_pp_without_embeddings.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nunomachado/Documents/MECD/S3/LN/LN23_project/MP2/realness_classification.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nunomachado/Documents/MECD/S3/LN/LN23_project/MP2/realness_classification.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X_pp_without_embeddings \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39m./processed_train/X_pp_without_embeddings.npy\u001b[39;49m\u001b[39m\"\u001b[39;49m, allow_pickle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nunomachado/Documents/MECD/S3/LN/LN23_project/MP2/realness_classification.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m yr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./processed_train/yr.npy\u001b[39m\u001b[39m\"\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nunomachado/Documents/MECD/S3/LN/LN23_project/MP2/realness_classification.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m ys \u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39m./processed_train/ys.npy\u001b[39m\u001b[39m\"\u001b[39m, allow_pickle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/LN23/lib/python3.8/site-packages/numpy/lib/npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    403\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 405\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    406\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './processed_train/X_pp_without_embeddings.npy'"
     ]
    }
   ],
   "source": [
    "X_pp_without_embeddings = np.load(\"./processed_train/X_pp_without_embeddings.npy\", allow_pickle=True)\n",
    "yr = np.load(\"./processed_train/yr.npy\", allow_pickle=True)\n",
    "ys =np.load(\"./processed_train/ys.npy\", allow_pickle=True)\n",
    "yr = yr.astype('float64')\n",
    "ys = ys.astype('float64')\n",
    "\n",
    "y = np.concatenate((np.transpose([yr]), np.transpose([ys])), axis=1)\n",
    "\n",
    "print(f\"type(X_pp_without_embeddings)-->{type(X_pp_without_embeddings)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 127] Impossível localizar o procedimento especificado",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tiago Miranda\\Desktop\\2nd_project_ln\\realness_classification.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#tokenizer\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m get_tokenizer\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# stop word removal\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\torchtext\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhub\u001b[39;00m \u001b[39mimport\u001b[39;00m _get_torch_home\n\u001b[0;32m      5\u001b[0m \u001b[39m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _extension  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m      8\u001b[0m _TEXT_BUCKET \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m _CACHE_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexpanduser(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(_get_torch_home(), \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\torchtext\\_extension.py:64\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m     \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m _init_extension()\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\torchtext\\_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _mod_utils\u001b[39m.\u001b[39mis_module_available(\u001b[39m\"\u001b[39m\u001b[39mtorchtext._torchtext\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     56\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mtorchtext C++ Extension is not found.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 58\u001b[0m _load_lib(\u001b[39m\"\u001b[39;49m\u001b[39mlibtorchtext\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     59\u001b[0m \u001b[39m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[39m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchtext\u001b[39;00m \u001b[39mimport\u001b[39;00m _torchtext\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\torchtext\\_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[1;34m(lib)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m path\u001b[39m.\u001b[39mexists():\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m torch\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mload_library(path)\n\u001b[0;32m     51\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\torch\\_ops.py:255\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    250\u001b[0m path \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_utils_internal\u001b[39m.\u001b[39mresolve_library_path(path)\n\u001b[0;32m    251\u001b[0m \u001b[39mwith\u001b[39;00m dl_open_guard():\n\u001b[0;32m    252\u001b[0m     \u001b[39m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[0;32m    253\u001b[0m     \u001b[39m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[39m# operators with the JIT.\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     ctypes\u001b[39m.\u001b[39;49mCDLL(path)\n\u001b[0;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloaded_libraries\u001b[39m.\u001b[39madd(path)\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\ctypes\\__init__.py:373\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_FuncPtr \u001b[39m=\u001b[39m _FuncPtr\n\u001b[0;32m    372\u001b[0m \u001b[39mif\u001b[39;00m handle \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m _dlopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name, mode)\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle \u001b[39m=\u001b[39m handle\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 127] Impossível localizar o procedimento especificado"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#tokenizer\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# stop word removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lemmazation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "df = pd.read_csv('train.txt', delimiter='\\t', header=None, names=[\"Label\", \"Review\"])\n",
    "df_test = pd.read_csv('test_just_reviews.txt', delimiter='\\t', header=None, names=[\"Review\"])\n",
    "\n",
    "#separating the labels into real/fake and positive/negative\n",
    "df['Realness'] = df['Label'].str[:-8]\n",
    "df['Sentiment'] = df['Label'].str[-8:]\n",
    "df = df.drop('Label', axis=1)\n",
    "\n",
    "# remove capitalized letters\n",
    "#df['Review'] = df['Review'].str.lower()\n",
    "#df['Review'] = tokenizer(df['Review'])\n",
    "data = df.to_numpy()\n",
    "Xt = df_test.to_numpy()[:,0]\n",
    "\n",
    "X = data[:,0]\n",
    "yr = data[:,1]\n",
    "ys = data[:,2]\n",
    "print(df)\n",
    "\n",
    "\n",
    "def pp_x_w_lemma_and_stop_word(X):\n",
    "\n",
    "    #tokenizing\n",
    "    tokenizer = get_tokenizer(\"basic_english\") \n",
    "    X = [tokenizer(x) for x in X]\n",
    "\n",
    "    print(f\"after tokenization: X[0]-->{X[0]}\")\n",
    "\n",
    "    # lemmazation\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # stop word removal (and punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # apply both stop word removal and lemmazation\n",
    "    X_stop = []\n",
    "    for x in X:\n",
    "        X_stop.append([wnl.lemmatize(i) for i in x if i not in stop_words and i not in string.punctuation])\n",
    "\n",
    "    #X_stop=np.array(X_stop)\n",
    "    print(f\"after stop word and lemma: X_stop[0]-->{X_stop[0]}\")\n",
    "\n",
    "    #print(f\"after stop word removal and lemmazation\\n\")\n",
    "    #print(f\"X_stop-->{X_stop}\")\n",
    "\n",
    "    #getting max numer of tokens in one review\n",
    "\n",
    "    max_words = 0\n",
    "    tam = []\n",
    "    for x in X_stop:\n",
    "        tam.append(len(x))\n",
    "        if len(x) > max_words:\n",
    "            max_words = len(x)\n",
    "\n",
    "    #max_words = 864\n",
    "    max_words=359\n",
    "\n",
    "    print(f\"max_words-->{max_words}\")\n",
    "\n",
    "    print(f\"type(X_stop[0])-->{type(X_stop[0])}\")\n",
    "    \n",
    "\n",
    "    #padding\n",
    "    X_stop = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X_stop]\n",
    "\n",
    "    print(f\"after padding: X_stop[0]-->{X_stop[0]}\")\n",
    "\n",
    "\n",
    "    return X_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pp = pp_x_w_lemma_and_stop_word(X)\n",
    "Xt_pp = pp_x_w_lemma_and_stop_word(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sheraton' 'wonderful' 'hotel' 'mom' 'flew' 'really' 'tired' 'decided'\n",
      " 'take' 'quick' 'nap' 'didnt' 'want' 'get' 'bed' 'absolutely' 'die'\n",
      " 'wanted' 'take' 'home' 'service' 'great' 'probably' 'one' 'biggest'\n",
      " 'biggest' 'hotel' 'ive' 'ever' 'stayed' 'really' 'nice' 'restaurant'\n",
      " 'inside' 'excellent' 'food' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n",
      " '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '']\n",
      "sheraton wonderful hotel mom flew really tired decided take quick nap didnt want get bed absolutely die wanted take home service great probably one biggest biggest hotel ive ever stayed really nice restaurant inside excellent food                                                                                                                                                                                                                                                                                                                                   \n"
     ]
    }
   ],
   "source": [
    "separator = ' ' \n",
    "\n",
    "result = separator.join(X_pp_without_embeddings[0])\n",
    "print(X_pp_without_embeddings[0])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Iterable over raw text documents expected, string object received.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tiago Miranda\\Desktop\\2nd_project_ln\\realness_classification.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tfidf_result \u001b[39m=\u001b[39m tfidf\u001b[39m.\u001b[39;49mfit_transform(result)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tiago%20Miranda/Desktop/2nd_project_ln/realness_classification.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(tfidf_result\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2079\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2072\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2073\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2074\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2075\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2076\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2077\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2078\u001b[0m )\n\u001b[1;32m-> 2079\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2080\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2081\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2082\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tiago Miranda\\anaconda3\\envs\\IST_DL21_Env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1317\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1313\u001b[0m \u001b[39m# We intentionally don't call the transform method to make\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[39m# fit_transform overridable without unwanted side effects in\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[39m# TfidfVectorizer.\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1318\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1319\u001b[0m     )\n\u001b[0;32m   1321\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1322\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_vocabulary()\n",
      "\u001b[1;31mValueError\u001b[0m: Iterable over raw text documents expected, string object received."
     ]
    }
   ],
   "source": [
    "tfidf_result = tfidf.fit_transform(result)\n",
    "print(tfidf_result.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
