{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nunomachado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import GloVe\n",
    "from torchtext.vocab import GloVe\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "#tokenizer\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# stop word removal\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', '?', 'a', 'b', 'c'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ={'a', 'b', 'c'}\n",
    "b = {'.', '?'}\n",
    "a.union(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'mk']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "string.punctuation\n",
    "a = ['hi', '.', 'mk']\n",
    "a = [word for word in a if word not in string.punctuation]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review   Realness Sentiment\n",
      "0     The sheraton was a wonderful hotel! When me an...   TRUTHFUL  POSITIVE\n",
      "1     We stayed at the Omni between Christmas and Ne...   TRUTHFUL  POSITIVE\n",
      "2     I was REALLY looking forward to a nice relaxin...  DECEPTIVE  NEGATIVE\n",
      "3     First let me say, I try not to be too critical...   TRUTHFUL  NEGATIVE\n",
      "4     The Ambassador East Hotel is a terrible place ...  DECEPTIVE  NEGATIVE\n",
      "...                                                 ...        ...       ...\n",
      "1395  I stayed here for 5 nights last summer. I book...   TRUTHFUL  NEGATIVE\n",
      "1396  Stayed here for 3 nights for a Bridgestone/Fir...   TRUTHFUL  POSITIVE\n",
      "1397  I am staying here now and actually am compelle...   TRUTHFUL  NEGATIVE\n",
      "1398  We stayed at this hotel with our two teenage d...   TRUTHFUL  NEGATIVE\n",
      "1399  The rooms were beautiful! The staff was friend...  DECEPTIVE  POSITIVE\n",
      "\n",
      "[1400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.txt', delimiter='\\t', header=None, names=[\"Label\", \"Review\"])\n",
    "df_test = pd.read_csv('test_just_reviews.txt', delimiter='\\t', header=None, names=[\"Review\"])\n",
    "\n",
    "#separating the labels into real/fake and positive/negative\n",
    "df['Realness'] = df['Label'].str[:-8]\n",
    "df['Sentiment'] = df['Label'].str[-8:]\n",
    "df = df.drop('Label', axis=1)\n",
    "\n",
    "# remove capitalized letters\n",
    "#df['Review'] = df['Review'].str.lower()\n",
    "#df['Review'] = tokenizer(df['Review'])\n",
    "data = df.to_numpy()\n",
    "Xt = df_test.to_numpy()[:,0]\n",
    "\n",
    "X = data[:,0]\n",
    "yr = data[:,1]\n",
    "ys = data[:,2]\n",
    "print(df)\n",
    "\n",
    "def pre_process_x(X):\n",
    "\n",
    "    #tokenizing\n",
    "    tokenizer = get_tokenizer(\"basic_english\") \n",
    "    X = [tokenizer(x) for x in X]\n",
    "\n",
    "    #getting max numer of tokens in one review\n",
    "\n",
    "    max_words = 0\n",
    "    tam = []\n",
    "    for x in X:\n",
    "        tam.append(len(x))\n",
    "        if len(x) > max_words:\n",
    "            max_words = len(x)\n",
    "\n",
    "    max_words = 864\n",
    "\n",
    "    print(f\"max_words-->{max_words}\")\n",
    "\n",
    "    #padding\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X]\n",
    "\n",
    "    #embeddings\n",
    "    X_emb = []\n",
    "    for token in X:\n",
    "        # tokenize and use glove word embedding\n",
    "        X_emb.append(np.array(global_vectors.get_vecs_by_tokens(token)))\n",
    "\n",
    "    X_emb = np.array(X_emb)\n",
    "    print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "\n",
    "    return X_emb\n",
    "\n",
    "\n",
    "def pp_x_w_lemma_and_stop_word(X):\n",
    "\n",
    "    #tokenizing\n",
    "    tokenizer = get_tokenizer(\"basic_english\") \n",
    "    X = [tokenizer(x) for x in X]\n",
    "\n",
    "    # stop word removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    X_stop = []\n",
    "    for x in X:\n",
    "        X_stop.append(np.array([i for i in x if i not in stop_words and i not in string.punctuation]))\n",
    "\n",
    "    X_stop=np.array(X_stop)\n",
    "    #print(f\"after stop word removal\\n\")\n",
    "    #print(f\"X_stop-->{X_stop}\")\n",
    "\n",
    "    # lemmazation\n",
    "\n",
    "\n",
    "    #getting max numer of tokens in one review\n",
    "\n",
    "    max_words = 0\n",
    "    tam = []\n",
    "    for x in X:\n",
    "        tam.append(len(x))\n",
    "        if len(x) > max_words:\n",
    "            max_words = len(x)\n",
    "\n",
    "    max_words = 864\n",
    "\n",
    "    print(f\"max_words-->{max_words}\")\n",
    "\n",
    "    #padding\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X]\n",
    "\n",
    "    #embeddings\n",
    "    X_emb = []\n",
    "    for token in X:\n",
    "        # tokenize and use glove word embedding\n",
    "        X_emb.append(np.array(global_vectors.get_vecs_by_tokens(token)))\n",
    "\n",
    "    X_emb = np.array(X_emb)\n",
    "    print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "\n",
    "    return X_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gy/3yklhl8s08v9hgg0b5hjg3hc0000gp/T/ipykernel_4718/1291336748.py:67: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_stop=np.array(X_stop)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after stop word removal\n",
      "\n",
      "X_stop-->[array(['sheraton', 'wonderful', 'hotel', 'mom', 'flew', 'really', 'tired',\n",
      "        'decided', 'take', 'quick', 'nap', 'didnt', 'want', 'get', 'beds',\n",
      "        'absolutely', 'die', 'wanted', 'take', 'home', 'service', 'great',\n",
      "        'probably', 'one', 'biggest', 'biggest', 'hotel', 'ive', 'ever',\n",
      "        'stayed', 'really', 'nice', 'restaurant', 'inside', 'excellent',\n",
      "        'food'], dtype='<U10')\n",
      " array(['stayed', 'omni', 'christmas', 'new', 'year', '2008', 'get',\n",
      "        'away', 'days', 'found', 'web', 'deals', 'got', 'penthouse',\n",
      "        'suite', 'cheap', 'year', 'checked', 'online', 'day', 'christmas',\n",
      "        'deal', 'also', 'included', 'parking', 'great', 'price', 'huge',\n",
      "        'suite', 'overlooking', 'downtown', 'chicago', 'best', 'location',\n",
      "        'walk', 'everywhere', '12', '11', 'yr', 'olds', 'loved',\n",
      "        'swimming', 'night', 'chocolate', 'cookies', 'pillows', 'wait',\n",
      "        'go', 'back'], dtype='<U11')\n",
      " array(['really', 'looking', 'forward', 'nice', 'relaxing', 'stay', 'end',\n",
      "        'long', 'vacation', 'unfortunately', 'moment', 'arrived', 'omni',\n",
      "        'staff', 'belligerent', 'extremely', 'rude', 'lost', 'reservation',\n",
      "        'refused', 'give', 'us', 'rate', 'booked', 'tired', 'would',\n",
      "        'gone', 'different', 'hotel', 'right', 'hindsight', '20/20',\n",
      "        'finally', 'got', 'checked', 'made', 'wait', 'needlessly', '45',\n",
      "        'minutes', 'lobby', 'decided', 'go', 'pool', 'tiny', 'kind',\n",
      "        'dirty', 'walked', 'way', 'suits', 'nothing', 'internet', 'access',\n",
      "        'really', 'slow', 'never', 'stay', 'save', 'headache', 'book',\n",
      "        'somewhere', 'else'], dtype='<U13')\n",
      " ...\n",
      " array(['staying', 'actually', 'compelled', 'write', 'review', 'fall',\n",
      "        'asleep', 'front', 'desk', 'staff', 'brief', 'one', 'chatting',\n",
      "        'friend', 'gossiping', 'checked', 'unprofessional', 'room',\n",
      "        'offered', 'high', 'floor', '5th', 'ha', 'checked', 'big', 'bag',\n",
      "        'grapes', 'left', 'behind', 'previous', 'guest', 'window', 'sill',\n",
      "        'previous', 'guests', 'hair', 'bathtub', 'done', 'called',\n",
      "        're-clean', 'room', 'made', 'offer', 'compensate', 'upgrade',\n",
      "        'even', 'bottle', 'wine', 'front', 'desk', 'clerk', 'suggested',\n",
      "        'come', 'get', 'key', 'another', 'room', 'like', 'time',\n",
      "        'disappointing', 'like', 'kimpton', 'hotels', 'kimpton', 'touch',\n",
      "        'member', 'btw', 'done', 'away', 'amenities', 'get', 'free',\n",
      "        'item', 'mini', 'bar'], dtype='<U14')\n",
      " array(['stayed', 'hotel', 'two', 'teenage', 'daughters', 'duing', 'part',\n",
      "        'business', 'part', 'family', 'fun', 'trip', 'july', 'impressed',\n",
      "        'location', 'within', 'walking', 'distance', 'great', 'restraunts',\n",
      "        'shopping', 'inlcuding', 'water', 'tower', 'place', 'great',\n",
      "        'mall', 'quite', 'disappointed', 'room', 'cleanlinees', 'size',\n",
      "        'bedspreads', 'filthy', 'thin', 'room', 'decor', 'dire', 'need',\n",
      "        'upgreade', 'bathroom', 'fan', 'really', 'smelled', 'badly',\n",
      "        'mold', 'mildew', 'shower', 'actually', 'mold', 'corners', 'tight',\n",
      "        'squeese', '4', 'people', 'especially', 'bathroom', 'beds',\n",
      "        'pillows', 'quite', 'uncomfortable', 'even', 'elevators', 'tiny',\n",
      "        'could', 'barely', 'fit', '6', 'people', 'thank', 'goodness',\n",
      "        'elevators', 'luckily', 'booked', 'room', 'priceline', 'would',\n",
      "        'never', 'paid', 'full', 'price', 'upside', 'great', 'location',\n",
      "        'also', 'beware-', 'cheaper', 'take', 'taxi', 'airport', 'airport',\n",
      "        'shuttle', 'millenium', 'promotes', 'airport', 'shuttle', 'take',\n",
      "        'cab'], dtype='<U13')\n",
      " array(['rooms', 'beautiful', 'staff', 'friendly', 'helpful', 'love',\n",
      "        'conrad', 'chicago', 'located', 'seemed', 'close', 'lot', 'places',\n",
      "        'wanted', 'visit', 'chicago', 'great', 'conrad', 'hotel', 'made',\n",
      "        'family', 'stay', 'much', 'exceptional'], dtype='<U11')            ]\n",
      "max_words-->864\n",
      "X_emb.shape-->(1400, 864, 300)\n"
     ]
    }
   ],
   "source": [
    "X_stop = pp_x_w_lemma_and_stop_word(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb = pre_process_x(X)\n",
    "Xt_emb = pre_process_x(Xt)\n",
    "\n",
    "######### y's pre processing\n",
    "for i in range(len(ys)):\n",
    "    if ys[i] == \"POSITIVE\":\n",
    "        ys[i] = 1\n",
    "    else:\n",
    "        ys[i] = 0\n",
    "\n",
    "    if yr[i] == \"TRUTHFUL\":\n",
    "        yr[i] = 1\n",
    "    else:\n",
    "        yr[i] = 0\n",
    "\n",
    "ys = np.array(ys)\n",
    "yr = np.array(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_emb)--><class 'numpy.ndarray'>\n",
      "type(Xt_emb)--><class 'numpy.ndarray'>\n",
      "type(ys)--><class 'numpy.ndarray'>\n",
      "type(yr)--><class 'numpy.ndarray'>\n",
      "\n",
      "X_emb.shape-->(1400, 864, 300)\n",
      "Xt_emb.shape-->(200, 864, 300)\n",
      "ys.shape-->(1400,)\n",
      "yr.shape-->(1400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(X_emb)-->{type(X_emb)}\")\n",
    "print(f\"type(Xt_emb)-->{type(Xt_emb)}\")\n",
    "print(f\"type(ys)-->{type(ys)}\")\n",
    "print(f\"type(yr)-->{type(yr)}\\n\")\n",
    "\n",
    "print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "print(f\"Xt_emb.shape-->{Xt_emb.shape}\")\n",
    "print(f\"ys.shape-->{ys.shape}\")\n",
    "print(f\"yr.shape-->{yr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"tam_min-->{np.min(tam)}\")\\nprint(f\"tam_MAX-->{np.max(tam)}\")\\nprint(f\"tam_avg-->{np.average(tam)}\")\\nprint(f\"tam_median-->{np.average(tam)}\")\\n\\n# distribuição do numero de palavras por review\\nplt.hist(tam)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(f\"tam_min-->{np.min(tam)}\")\n",
    "print(f\"tam_MAX-->{np.max(tam)}\")\n",
    "print(f\"tam_avg-->{np.average(tam)}\")\n",
    "print(f\"tam_median-->{np.average(tam)}\")\n",
    "\n",
    "# distribuição do numero de palavras por review\n",
    "plt.hist(tam)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete cache\n",
    "del(global_vectors)\n",
    "#save processed numpy arrays\n",
    "np.save(\"./processed_train/X_emb.npy\", X_emb)\n",
    "np.save(\"./processed_train/Xt_emb.npy\", Xt_emb)\n",
    "np.save(\"./processed_train/ys.npy\", ys)\n",
    "np.save(\"./processed_train/yr.npy\", yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savez_compressed('processed_train/all.npz', X_emb=X_emb, Xt_emb=Xt_emb, ys=ys, yr=yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LN23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
