{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nunomachado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nunomachado/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "# import GloVe\n",
    "from torchtext.vocab import GloVe\n",
    "global_vectors = GloVe(name='840B', dim=300)\n",
    "\n",
    "#tokenizer\n",
    "import torchtext\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# stop word removal\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# lemmazation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-pocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review   Realness Sentiment\n",
      "0     The sheraton was a wonderful hotel! When me an...   TRUTHFUL  POSITIVE\n",
      "1     We stayed at the Omni between Christmas and Ne...   TRUTHFUL  POSITIVE\n",
      "2     I was REALLY looking forward to a nice relaxin...  DECEPTIVE  NEGATIVE\n",
      "3     First let me say, I try not to be too critical...   TRUTHFUL  NEGATIVE\n",
      "4     The Ambassador East Hotel is a terrible place ...  DECEPTIVE  NEGATIVE\n",
      "...                                                 ...        ...       ...\n",
      "1395  I stayed here for 5 nights last summer. I book...   TRUTHFUL  NEGATIVE\n",
      "1396  Stayed here for 3 nights for a Bridgestone/Fir...   TRUTHFUL  POSITIVE\n",
      "1397  I am staying here now and actually am compelle...   TRUTHFUL  NEGATIVE\n",
      "1398  We stayed at this hotel with our two teenage d...   TRUTHFUL  NEGATIVE\n",
      "1399  The rooms were beautiful! The staff was friend...  DECEPTIVE  POSITIVE\n",
      "\n",
      "[1400 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('train.txt', delimiter='\\t', header=None, names=[\"Label\", \"Review\"])\n",
    "df_test = pd.read_csv('test_just_reviews.txt', delimiter='\\t', header=None, names=[\"Review\"])\n",
    "\n",
    "#separating the labels into real/fake and positive/negative\n",
    "df['Realness'] = df['Label'].str[:-8]\n",
    "df['Sentiment'] = df['Label'].str[-8:]\n",
    "df = df.drop('Label', axis=1)\n",
    "\n",
    "# remove capitalized letters\n",
    "#df['Review'] = df['Review'].str.lower()\n",
    "#df['Review'] = tokenizer(df['Review'])\n",
    "data = df.to_numpy()\n",
    "Xt = df_test.to_numpy()[:,0]\n",
    "\n",
    "X = data[:,0]\n",
    "yr = data[:,1]\n",
    "ys = data[:,2]\n",
    "print(df)\n",
    "\n",
    "def pre_process_x(X):\n",
    "\n",
    "    #tokenizing\n",
    "    tokenizer = get_tokenizer(\"basic_english\") \n",
    "    X = [tokenizer(x) for x in X]\n",
    "\n",
    "    #getting max numer of tokens in one review\n",
    "\n",
    "    max_words = 0\n",
    "    tam = []\n",
    "    for x in X:\n",
    "        tam.append(len(x))\n",
    "        if len(x) > max_words:\n",
    "            max_words = len(x)\n",
    "\n",
    "    max_words = 864\n",
    "\n",
    "    print(f\"max_words-->{max_words}\")\n",
    "\n",
    "    #padding\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X]\n",
    "\n",
    "    #embeddings\n",
    "    X_emb = []\n",
    "    for token in X:\n",
    "        # tokenize and use glove word embedding\n",
    "        X_emb.append(np.array(global_vectors.get_vecs_by_tokens(token)))\n",
    "\n",
    "    X_emb = np.array(X_emb)\n",
    "    print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "\n",
    "    return X_emb\n",
    "\n",
    "\n",
    "def pp_x_w_lemma_and_stop_word(X):\n",
    "\n",
    "    #tokenizing\n",
    "    tokenizer = get_tokenizer(\"basic_english\") \n",
    "    X = [tokenizer(x) for x in X]\n",
    "\n",
    "    print(f\"after tokenization: X[0]-->{X[0]}\")\n",
    "\n",
    "    # lemmazation\n",
    "    wnl = WordNetLemmatizer()\n",
    "\n",
    "    # stop word removal (and punctuation)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    # apply both stop word removal and lemmazation\n",
    "    X_stop = []\n",
    "    for x in X:\n",
    "        X_stop.append([wnl.lemmatize(i) for i in x if i not in stop_words and i not in string.punctuation])\n",
    "\n",
    "    #X_stop=np.array(X_stop)\n",
    "    print(f\"after stop word and lemma: X_stop[0]-->{X_stop[0]}\")\n",
    "\n",
    "    #print(f\"after stop word removal and lemmazation\\n\")\n",
    "    #print(f\"X_stop-->{X_stop}\")\n",
    "\n",
    "    #getting max numer of tokens in one review\n",
    "\n",
    "    max_words = 0\n",
    "    tam = []\n",
    "    for x in X_stop:\n",
    "        tam.append(len(x))\n",
    "        if len(x) > max_words:\n",
    "            max_words = len(x)\n",
    "\n",
    "    #max_words = 864\n",
    "    max_words=359\n",
    "\n",
    "    print(f\"max_words-->{max_words}\")\n",
    "\n",
    "    print(f\"type(X_stop[0])-->{type(X_stop[0])}\")\n",
    "    \n",
    "\n",
    "    #padding\n",
    "    X_stop = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X_stop]\n",
    "\n",
    "    print(f\"after padding: X_stop[0]-->{X_stop[0]}\")\n",
    "\n",
    "    #embeddings\n",
    "    X_emb = []\n",
    "    for token in X_stop:\n",
    "        # tokenize and use glove word embedding\n",
    "        X_emb.append(np.array(global_vectors.get_vecs_by_tokens(token)))\n",
    "\n",
    "    X_emb = np.array(X_emb)\n",
    "\n",
    "    print(f\"after embedding: X_emb[0]-->{X_emb[0]}\")\n",
    "    print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "\n",
    "    return X_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after tokenization: X[0]-->['the', 'sheraton', 'was', 'a', 'wonderful', 'hotel', '!', 'when', 'me', 'and', 'my', 'mom', 'flew', 'in', 'we', 'were', 'really', 'tired', 'so', 'we', 'decided', 'to', 'take', 'a', 'quick', 'nap', '.', 'we', 'didnt', 'want', 'to', 'get', 'up', '!', 'the', 'beds', 'are', 'absolutely', 'to', 'die', 'for', '.', 'i', 'wanted', 'to', 'take', 'it', 'home', 'with', 'me', '.', 'the', 'service', 'was', 'great', 'and', 'this', 'was', 'probably', 'one', 'of', 'the', 'biggest', 'if', 'not', 'the', 'biggest', 'hotel', 'ive', 'ever', 'stayed', 'in', '.', 'they', 'had', 'a', 'really', 'nice', 'restaurant', 'inside', 'with', 'excellent', 'food', '.']\n",
      "after stop word and lemma: X_stop[0]-->['sheraton', 'wonderful', 'hotel', 'mom', 'flew', 'really', 'tired', 'decided', 'take', 'quick', 'nap', 'didnt', 'want', 'get', 'bed', 'absolutely', 'die', 'wanted', 'take', 'home', 'service', 'great', 'probably', 'one', 'biggest', 'biggest', 'hotel', 'ive', 'ever', 'stayed', 'really', 'nice', 'restaurant', 'inside', 'excellent', 'food']\n",
      "max_words-->359\n",
      "type(X_stop[0])--><class 'list'>\n",
      "after padding: X_stop[0]-->['sheraton', 'wonderful', 'hotel', 'mom', 'flew', 'really', 'tired', 'decided', 'take', 'quick', 'nap', 'didnt', 'want', 'get', 'bed', 'absolutely', 'die', 'wanted', 'take', 'home', 'service', 'great', 'probably', 'one', 'biggest', 'biggest', 'hotel', 'ive', 'ever', 'stayed', 'really', 'nice', 'restaurant', 'inside', 'excellent', 'food', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "after embedding: X_emb[0]-->[[ 0.52609   0.012913  0.43838  ... -0.18133  -0.27765   0.22226 ]\n",
      " [ 0.1079    0.48507  -0.25341  ... -0.10304  -0.13481   0.30559 ]\n",
      " [ 0.42714   0.12711   0.33711  ...  0.11373   0.11649   0.89665 ]\n",
      " ...\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n",
      "X_emb.shape-->(1400, 359, 300)\n",
      "after tokenization: X[0]-->['my', 'family', 'and', 'i', 'stayed', 'here', 'while', 'we', 'were', 'visiting', 'chicago', '.', 'this', 'was', 'a', 'perfect', 'location', 'to', 'many', 'of', 'the', 'things', 'we', 'wanted', 'to', 'do', 'and', 'was', 'easy', 'to', 'get', 'around', '.', 'it', 'was', 'in', 'a', 'safe', 'neighborhood', 'and', 'was', 'great', 'accomodations', '.', 'the', 'staff', 'was', 'friendly', 'and', 'went', 'the', 'extra', 'mile', 'to', 'make', 'sure', 'we', 'had', 'everything', 'we', 'needed', '.', 'the', 'beds', 'were', 'comfortable', '.', 'we', 'would', 'definitely', 'stay', 'here', 'again', '.']\n",
      "after stop word and lemma: X_stop[0]-->['family', 'stayed', 'visiting', 'chicago', 'perfect', 'location', 'many', 'thing', 'wanted', 'easy', 'get', 'around', 'safe', 'neighborhood', 'great', 'accomodations', 'staff', 'friendly', 'went', 'extra', 'mile', 'make', 'sure', 'everything', 'needed', 'bed', 'comfortable', 'would', 'definitely', 'stay']\n",
      "max_words-->380\n",
      "type(X_stop[0])--><class 'list'>\n",
      "after padding: X_stop[0]-->['family', 'stayed', 'visiting', 'chicago', 'perfect', 'location', 'many', 'thing', 'wanted', 'easy', 'get', 'around', 'safe', 'neighborhood', 'great', 'accomodations', 'staff', 'friendly', 'went', 'extra', 'mile', 'make', 'sure', 'everything', 'needed', 'bed', 'comfortable', 'would', 'definitely', 'stay', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n",
      "after embedding: X_emb[0]-->[[-0.012915  0.37364  -0.13332  ... -0.3807   -0.062887 -0.22051 ]\n",
      " [ 0.52173  -0.052083 -0.005499 ...  0.26181   0.11699   0.0266  ]\n",
      " [ 0.61637  -0.28507  -0.64867  ...  0.11589   0.065659  0.045001]\n",
      " ...\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...  0.        0.        0.      ]]\n",
      "X_emb.shape-->(200, 380, 300)\n"
     ]
    }
   ],
   "source": [
    "X_pp = pp_x_w_lemma_and_stop_word(X)\n",
    "Xt_pp = pp_x_w_lemma_and_stop_word(Xt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_words-->864\n",
      "X_emb.shape-->(1400, 864, 300)\n",
      "max_words-->864\n",
      "X_emb.shape-->(200, 864, 300)\n"
     ]
    }
   ],
   "source": [
    "X_emb = pre_process_x(X)\n",
    "Xt_emb = pre_process_x(Xt)\n",
    "\n",
    "######### y's pre processing\n",
    "for i in range(len(ys)):\n",
    "    if ys[i] == \"POSITIVE\":\n",
    "        ys[i] = 1\n",
    "    else:\n",
    "        ys[i] = 0\n",
    "\n",
    "    if yr[i] == \"TRUTHFUL\":\n",
    "        yr[i] = 1\n",
    "    else:\n",
    "        yr[i] = 0\n",
    "\n",
    "ys = np.array(ys)\n",
    "yr = np.array(yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(X_emb)--><class 'numpy.ndarray'>\n",
      "type(Xt_emb)--><class 'numpy.ndarray'>\n",
      "type(ys)--><class 'numpy.ndarray'>\n",
      "type(yr)--><class 'numpy.ndarray'>\n",
      "\n",
      "X_emb.shape-->(1400, 864, 300)\n",
      "Xt_emb.shape-->(200, 864, 300)\n",
      "ys.shape-->(1400,)\n",
      "yr.shape-->(1400,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"type(X_emb)-->{type(X_emb)}\")\n",
    "print(f\"type(Xt_emb)-->{type(Xt_emb)}\")\n",
    "print(f\"type(ys)-->{type(ys)}\")\n",
    "print(f\"type(yr)-->{type(yr)}\\n\")\n",
    "\n",
    "print(f\"X_emb.shape-->{X_emb.shape}\")\n",
    "print(f\"Xt_emb.shape-->{Xt_emb.shape}\")\n",
    "print(f\"ys.shape-->{ys.shape}\")\n",
    "print(f\"yr.shape-->{yr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint(f\"tam_min-->{np.min(tam)}\")\\nprint(f\"tam_MAX-->{np.max(tam)}\")\\nprint(f\"tam_avg-->{np.average(tam)}\")\\nprint(f\"tam_median-->{np.average(tam)}\")\\n\\n# distribuição do numero de palavras por review\\nplt.hist(tam)\\nplt.show()\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "print(f\"tam_min-->{np.min(tam)}\")\n",
    "print(f\"tam_MAX-->{np.max(tam)}\")\n",
    "print(f\"tam_avg-->{np.average(tam)}\")\n",
    "print(f\"tam_median-->{np.average(tam)}\")\n",
    "\n",
    "# distribuição do numero de palavras por review\n",
    "plt.hist(tam)\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete cache\n",
    "del(global_vectors)\n",
    "#save processed numpy arrays\n",
    "np.save(\"./processed_train/X_emb.npy\", X_emb)\n",
    "np.save(\"./processed_train/Xt_emb.npy\", Xt_emb)\n",
    "np.save(\"./processed_train/ys.npy\", ys)\n",
    "np.save(\"./processed_train/yr.npy\", yr)\n",
    "\n",
    "## more pre processed, lemmazation and stop word removal\n",
    "\n",
    "np.save(\"./processed_train/X_pp.npy\", X_pp)\n",
    "np.save(\"./processed_train/Xt_pp.npy\", Xt_pp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LN23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
